{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEl+nwnI3oBQ2/a/FNFuTq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evgeny-kolonsky/gaza-news/blob/main/WSJ_news_crawl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wall Street Journal articles counter"
      ],
      "metadata": {
        "id": "7LONLes-HldL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eugene Kolonsky 2022-24\n",
        "# script for news headlines data scrapping from Wall Street Journal web site"
      ],
      "metadata": {
        "id": "jWQgO8NAHstF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CouEjql3Hq-0",
        "outputId": "23495d2e-19dd-4514-e3bb-d4932cdd0afe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.6.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.14.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=0f57386d0d9e6e64c199851378a84b67c6185bcc5c4b612677ca07af07a230ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=a4d4bd92861bca12255dde92eb99b723a45034d90635e7142818719796ee2898\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=d0df26f511d8f5d3f0845029e8b2466621fef3a7b65f7bb07cfcb6e2f9ff6c09\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=d4c07d499daf594fc9bb434d5f7752d3c7367cbf3a0b0f3ef9cfdd49023073c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFrHsr89pQIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0f5374-86c7-4b2d-9a1a-04ec0ad3a4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "import urllib, urllib.request, time, datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import date, timedelta\n",
        "from newspaper import Article\n",
        "import json\n",
        "\n",
        "\n",
        "site = 'https://www.wsj.com/news/archive/'\n",
        "\n",
        "# set here start date, final date and filename for data saving\n",
        "\n",
        "\n",
        "FORMAT = '%Y/%m/%d'\n",
        "start_day = datetime.datetime.strptime('2023/10/01', FORMAT)\n",
        "fin_day   = datetime.datetime.strptime('2024/06/13', FORMAT)\n",
        "filename = 'wsj-news-middleeast.txt'\n",
        "\n",
        "def get_page(day, page_num):\n",
        "  url = site + day\n",
        "  url_page = url + '/?page=' + str(page_num)\n",
        "\n",
        "  try:\n",
        "    req = urllib.request.Request(url_page, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    resp = urllib.request.urlopen(req, timeout=2)\n",
        "    respData = resp.read().decode('utf-8')\n",
        "  except Exception as e:\n",
        "    respData = ''\n",
        "  return respData\n",
        "\n",
        "def get_links(html_page):\n",
        "  links = {}\n",
        "  soup = BeautifulSoup(html_page)\n",
        "  for link in soup.find_all('a'):\n",
        "      href = str(link.get('href'))\n",
        "      title = str(link.string)\n",
        "      if  'world/middle-east' in href and '?mod' not in href:\n",
        "        #print(href)\n",
        "        links[href] = title\n",
        "  return links\n",
        "\n",
        "def get_day(day):\n",
        "  links = {}\n",
        "  for page_num in range(1,10):\n",
        "    p = get_page(day, page_num)\n",
        "    more_links = get_links(p)\n",
        "    if any(link in links.keys() for link in more_links.keys()) or len(more_links)==0:\n",
        "        break\n",
        "    else:\n",
        "        links.update(more_links)\n",
        "  return links\n",
        "\n",
        "# clear file\n",
        "file = open(filename, 'w')\n",
        "file.close()\n",
        "\n",
        "\n",
        "day = start_day\n",
        "while day < fin_day:\n",
        "  strday = day.strftime(FORMAT)\n",
        "  urls = get_day(strday)\n",
        "  lines = ''\n",
        "  for url in urls:\n",
        "    title = urls[url]\n",
        "    publisher = 'WSJ'\n",
        "    line = f'{day:%Y-%m-%d}\\t{publisher}\\t{title}\\t{url}\\n'\n",
        "    lines += line\n",
        "    print('.', end='')\n",
        "    with open(filename, 'a') as f:\n",
        "      f.write(line)\n",
        "  day = day + timedelta(days=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OBcS9S6MUCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}